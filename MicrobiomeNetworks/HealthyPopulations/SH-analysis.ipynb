{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from statistics import stdev, mean\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import os\n",
    "import subprocess as subp\n",
    "import re\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.stats import multitest\n",
    "\n",
    "from skbio.stats.composition import ilr, clr\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataTrans(df):\n",
    "    \"\"\"\n",
    "    Function to conduct CLR transformation on given data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe containing the data for transformation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe with log-ratio transformed data\n",
    "    \"\"\"\n",
    "    \n",
    "    wdf = df.drop(\"Group\", axis=1).copy().fillna(0) #Working DF\n",
    "    \n",
    "    #Replace 0's with very small numbers\n",
    "    trans_df = wdf.set_index(\"SampleName\").apply(lambda x:x.replace(0, 1e-10))\n",
    "    \n",
    "    #Tranform the data and turn into a DF\n",
    "    clr_df = pd.DataFrame(clr(trans_df),\\\n",
    "                      columns=list(trans_df.columns),\\\n",
    "                      index=list(trans_df.index.values))\n",
    "\n",
    "    #Add the group labels\n",
    "    clr_df = clr_df.join(df.set_index(\"SampleName\").Group)\n",
    "    return clr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RFC_FI(trans_df, n, top_model_name, save, top_n=20, out=False, dpi=200):\n",
    "    \"\"\"Function to run a random forest classifier on transformed data n times and find the feature importances\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    trans_df: Dataframe containing transformed data\n",
    "    n: Integer specifying how many times to re-run the classifier\n",
    "    top_model_name: String specifying name to save the top-performing model as (without suffix)\n",
    "    top_n: Integer specifying the top-n feature importances to show\n",
    "    save: Boolean specifying whether to save the image or not\n",
    "    out: String to save image as\n",
    "    dpi: Integer specifying dpi of saved image\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    F-1 Scores for each run\n",
    "    Image for top-n feature importances\n",
    "    Importances returned as a global dataframe named 'imps'\n",
    "    F1-scores for every run stored in a global dict named 'totals'\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from collections import defaultdict\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "    \n",
    "    d = defaultdict(LabelEncoder)\n",
    "    \n",
    "    edf = trans_df.dropna().apply(lambda x: d[x.name].fit_transform(x)) #Encoded DF\n",
    "\n",
    "    x = edf.drop([\"Group\"], axis=1)\n",
    "    y = edf.Group.values\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.3)\n",
    "    \n",
    "    #Create dictionaries to hold the values for Group f-1 scores\n",
    "    a_runs = {} #Runs for group a; contains F-1 scores\n",
    "    b_runs = {} #Runs for group b; contains F-1 scores\n",
    "\n",
    "    global totals\n",
    "    totals = {0:0}\n",
    "\n",
    "    runs = 0\n",
    "\n",
    "    #START LOOP\n",
    "\n",
    "    print(\"Starting RFC\\n\")\n",
    "    \n",
    "    time0 = time.time()\n",
    "    \n",
    "    for i in range(n):\n",
    "        runs +=1\n",
    "        time1 = time.time()\n",
    "\n",
    "        rf = rfc(n_estimators=500, oob_score=True)\n",
    "        rf.fit(xtrain, ytrain)\n",
    "        predicts = rf.predict(xtest)\n",
    "\n",
    "        a_runs[i] = (classification_report(ytest, predicts).split(sep=\"      \"))[6].strip()\n",
    "        b_runs[i] = (classification_report(ytest, predicts).split(sep=\"      \"))[11].strip()\n",
    "\n",
    "        s = rf.score(xtest, ytest)\n",
    "\n",
    "        if s > max(totals.values()) :\n",
    "            pickle.dump(rf, open(top_model_name+'.sav', 'wb'))\n",
    "\n",
    "        totals[i] = s\n",
    "\n",
    "        print(\" Run {} Completed -- Score = {} -- TimeForLoop = {} seconds\".format(\n",
    "            runs, s, round((time.time() - time1),4)), end=\"\\r\")\n",
    "\n",
    "    #END LOOP\n",
    "    print(\"\\nRFC Complete. Top model saved as '{}.sav'.\".format(top_model_name))\n",
    "    \n",
    "    #Find feature importances\n",
    "#     global rf1\n",
    "    rf1 = pickle.load(open(top_model_name+'.sav', 'rb')) #Load the highest performing classifier\n",
    "\n",
    "    #Create a dataframe that houses the feature importances\n",
    "    global imps\n",
    "    imps = pd.DataFrame(rf1.feature_importances_, columns=['Importance'], index=xtrain.columns)\n",
    "    imps.sort_values('Importance', ascending=False, inplace=True)\n",
    "    imps.Importance = imps.Importance*100\n",
    "    imps.rename(columns={'Importance':'Importance %'}, inplace=True)\n",
    "    new_labels = []\n",
    "    for i in imps.index:\n",
    "        new_labels.append(\"\\n\\n\\n\"+i.replace(\"_strain\", \"\\nstrain_\"))\n",
    "    imps[\"GraphLabels\"] = new_labels\n",
    "    \n",
    "    print(f\"Top scoring model F1-score: {round(np.max(list(totals.values())), 3)}\")\n",
    "\n",
    "\n",
    "    imps.set_index(\"GraphLabels\")[:top_n].plot(kind='bar', legend=False, figsize=(20,16),\\\n",
    "                      color='gold', linewidth=0.7, edgecolor='k')\n",
    "    plt.title('Top-{} Feature Importances\\n'.format(top_n), size=22)\n",
    "    plt.xlabel('\\nFeature Name', size=15)\n",
    "    plt.ylabel('Percent Feature Importances\\n', size=17)\n",
    "    plt.yticks(size=15)\n",
    "    plt.xticks(rotation=45, ha='right', size=17)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        if out:\n",
    "            plt.savefig(out, dpi=dpi)\n",
    "            print(\"Image saved as {}\".format(out))\n",
    "        else:\n",
    "            print(\"No image name specified in 'out' variable. No image saved!\")\n",
    "            \n",
    "    \n",
    "    print(\"\\n\"+60*\"-\")\n",
    "    global class_report\n",
    "    class_report = classification_report(ytest, predicts, target_names=list(d['Group'].classes_))\n",
    "    print(class_report)\n",
    "    print(60*\"-\", \"\\n\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "udf = pd.read_csv(\"DataFiles/allCohorts_90PrevSpecies.csv\").rename(columns={\"Cohort\":\"Group\"})\n",
    "uclr = DataTrans(udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RFC and store feature importances\n",
    "# RFC_FI(uclr, n=50, top_model_name=\"CoreUnion5Test\",\\\n",
    "#        top_n=10, save=False)\n",
    "\n",
    "\n",
    "# Save totals to a csv\n",
    "# pd.DataFrame.from_dict(totals, orient='index', columns=[\"F1-Score\"]).to_csv(\"totals.csv\")\n",
    "\n",
    "# Save imps to a csv\n",
    "# imps.to_csv(\"imps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_nb\n",
    "\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeNet(F, thresh=0.01):\n",
    "    \"\"\"\n",
    "    Function to make the network within the notebook\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    F: File path\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    nx.Graph object\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Import matrix\n",
    "    try:\n",
    "        df = pd.read_csv(F, index_col=\"Unnamed: 0\")\n",
    "    except:\n",
    "        try:\n",
    "            df = pd.read_csv(F, index_col=\"Names\")\n",
    "        except:\n",
    "            df = pd.read_csv(F)\n",
    "\n",
    "\n",
    "    # Fix the names to fit in the nodes\n",
    "#     new_names = list(df.apply(lambda x:x.name.replace(\"_\", \"\\n\")).values)\n",
    "\n",
    "#     df[\"Index\"] = new_names\n",
    "#     df.set_index(\"Index\", inplace=True)\n",
    "#     df.columns =  new_names\n",
    "\n",
    "    # Create an empty graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Iterate through the data to extract the nodes, edges, and weights\n",
    "    for n1 in df.columns:\n",
    "        for n2 in df.index:\n",
    "            if abs(df.loc[n2,n1]) >= thresh and n2 != n1:\n",
    "                if abs(df.loc[n2,n1]) != 0:\n",
    "                    # Add weighted edge (wt) between node1 (n1) and node2 (n2)\n",
    "                    G.add_edge(n1,n2, weight = df.loc[n2,n1])\n",
    "                    \n",
    "    print(f\"{F.split('_')[0]}: N: {len(G.nodes)} E:{len(G.edges)}\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ViewTops(G, top_list, print_neighbors=False):\n",
    "    \"\"\"\n",
    "    Function to find the average edge weights of the top_list of nodes compared to other nodes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G: nx.Graph object\n",
    "    top_list: list of nodes\n",
    "    print_neighbors: Boolean to print out each node name and it's number of direct neighbors\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Print-outs\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    weights = []\n",
    "    non_weights = []\n",
    "        \n",
    "    for n in G.nodes:\n",
    "        if print_neighbors:\n",
    "            # Make the print out nicer\n",
    "#             n = n.replace(\"\\n\", \"_\")\n",
    "            string = f\"{n} neighbors\"\n",
    "            print(\"-\" * len(string))\n",
    "            print(string)\n",
    "            print(\"-\" * len(string))\n",
    "\n",
    "        # Correct the node names    \n",
    "#         n = n.replace(\"_\", \"\\n\")\n",
    "\n",
    "        # Extract the neighbors\n",
    "        try:\n",
    "#             print(\"Node: \" + n.replace(\"\\n\", \"_\") + \" neighbors: \" + str(len(G[n])))\n",
    "            for k,v in G[n].items():\n",
    "                if n in top_list:\n",
    "                    if print_neighbors:\n",
    "#                         k = k.replace(\"\\n\", \"_\")\n",
    "                        print(f\"{k}: {v}\")\n",
    "                    else:\n",
    "                        weights.append(v[\"weight\"])\n",
    "                else:\n",
    "                    non_weights.append(v[\"weight\"])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    deg_df = pd.DataFrame.from_dict(dict(G.degree), orient='index', columns=[\"degree\"])\n",
    "    found = {} # {node:degree}\n",
    "         \n",
    "    # Correct the node names\n",
    "    for n in top_list:\n",
    "        # Extract the degrees\n",
    "        try:\n",
    "            found[n] = int(deg_df.loc[n])\n",
    "            \n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    # avg degree of non-top list nodes\n",
    "    non_avg = deg_df.loc[[x for x in deg_df.index if x not in found]].degree.mean()\n",
    "        \n",
    "    results_cols = [\"Group\", \"Average edge weight\", \"Average degree\"]\n",
    "    results = [[\"Conserved\", round(np.mean(weights), 4), round(np.mean(list(found.values())), 3)],\n",
    "              [\"Non-conserved\", round(np.mean(non_weights), 4), round(non_avg, 3)]]\n",
    "    \n",
    "    # Print the results\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(results, headers=results_cols))\n",
    "        \n",
    "#     print(f\"{len(weights)} nodes are directly connected to top-nodes\")\n",
    "#     print(f\"Average edge weight of top nodes: {round(np.mean(weights), 4)}\")\n",
    "    \n",
    "#     print(f\"{len(non_weights)} nodes are NOT directly connected to top-nodes\")\n",
    "#     print(f\"Average edge weight of non-top nodes: {round(np.mean(non_weights), 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "american: N: 97 E:338\n",
      "european: N: 149 E:386\n",
      "japanese: N: 117 E:274\n",
      "indian: N: 104 E:273\n"
     ]
    }
   ],
   "source": [
    "amerG = MakeNet(\"DataFiles/american_90_clr_resultsPCorBootOmega.csv\")\n",
    "euroG = MakeNet(\"DataFiles/european_90_clr_resultsPCorBootOmega.csv\")\n",
    "japnG = MakeNet(\"DataFiles/japanese_90_clr_resultsPCorBootOmega.csv\")\n",
    "indnG = MakeNet(\"DataFiles/indian_90_clr_resultsPCorBootOmega.csv\")\n",
    "\n",
    "GraphsDict = {\"European\":euroG, \"Indian\":indnG, \"HMP\":amerG, \"Japanese\":japnG}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "Top-20 FI nodes\n",
      "================\n",
      "\n",
      "\n",
      "European network\n",
      "-----------------\n",
      "Group            Average edge weight    Average degree\n",
      "-------------  ---------------------  ----------------\n",
      "Conserved                     0.077              3.214\n",
      "Non-conserved                 0.0686             5.385\n",
      "\n",
      "\n",
      "Indian network\n",
      "---------------\n",
      "Group            Average edge weight    Average degree\n",
      "-------------  ---------------------  ----------------\n",
      "Conserved                     0.0851             3\n",
      "Non-conserved                 0.0702             5.489\n",
      "\n",
      "\n",
      "HMP network\n",
      "------------\n",
      "Group            Average edge weight    Average degree\n",
      "-------------  ---------------------  ----------------\n",
      "Conserved                     0.0582             5.667\n",
      "Non-conserved                 0.0533             7.102\n",
      "\n",
      "\n",
      "Japanese network\n",
      "-----------------\n",
      "Group            Average edge weight    Average degree\n",
      "-------------  ---------------------  ----------------\n",
      "Conserved                     0.0727             2.786\n",
      "Non-conserved                 0.0703             4.942\n"
     ]
    }
   ],
   "source": [
    "# Import the top-20 FI's\n",
    "df20 = pd.read_csv(\"DataFiles/Top20_FI_list.csv\")\n",
    "top_20 = list(df20[df20.ID == \"Top-20 FI\"][\"Bacteria\"])\n",
    "\n",
    "\n",
    "title = \"Top-20 FI nodes\"\n",
    "print(\"=\" * (len(title)+1))\n",
    "print(title)\n",
    "print(\"=\" * (len(title)+1))\n",
    "\n",
    "for g in GraphsDict:\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    string = f\"{g} network\"\n",
    "    print(string)\n",
    "    print(\"-\" * (len(string)+1))\n",
    "    \n",
    "    ViewTops(GraphsDict[g], top_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DrawPCA(df, var, save=False, dpi=200, plot_title=None, out=False, loadings=False, color_pal=None, order_list=None):\n",
    "    \"\"\"\n",
    "    Function to draw PCA using the given data\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    df: Transformed dataframe\n",
    "    var: Variable of interest\n",
    "    plot_title: String to be title of plot\n",
    "    save: Boolean to save the image\n",
    "    dpi: Integer specifying dpi of saved image\n",
    "    out: String to save the image as\n",
    "    loadings: Boolean to determine if contributions to Component 1 are displayed\n",
    "    color_pal: Name of Seaborn color palette\n",
    "    order_list = list of the order to color the groups (diseased should be in the front)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Image of PCA\n",
    "    Series of loadings (optional)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import the neccessary modules\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    trans_df = df.copy()\n",
    "    \n",
    "    #Split the data up for the PCA\n",
    "    xp = trans_df.drop(var, axis=1).values\n",
    "    yp = trans_df[var].values\n",
    "\n",
    "    #Fit the PCA\n",
    "    pca = PCA(n_components=10)\n",
    "    trans_pca = pca.fit_transform(xp)\n",
    "\n",
    "    #Store the values in a DF for future use\n",
    "    global pcdf\n",
    "    pcdf = pd.DataFrame(data=trans_pca.T[:5].T, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"],\\\n",
    "                       index= list(trans_df.index))\n",
    "    pcdf[var] = yp\n",
    "\n",
    "    #Extract the explained variances\n",
    "    pc1_var = round(pca.explained_variance_ratio_[0]*100, 2)\n",
    "    pc2_var = round(pca.explained_variance_ratio_[1]*100, 2)\n",
    "\n",
    "    #Graph the PCA\n",
    "    #Create a color palette\n",
    "    sc = [\"#ff0000\", \"#70b9fd\", \"#ffd3d9\", \"#FF8B00\", \"#FF70BC\",\\\n",
    "          \"#ffffff\", \"#c97ec9\", \"#00ea04\", \"#4230ff\", \"#000000\"]\n",
    "    scp = sns.color_palette(sc)\n",
    "    scp2 = scp[:len(np.unique(yp))]\n",
    "    \n",
    "    if color_pal == None:\n",
    "        color_pal = scp2\n",
    "        \n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    sns.scatterplot(data=pcdf.sort_values(by=var), x='PC1', y='PC2', hue=var, hue_order = order_list, legend='full',\\\n",
    "                    linewidth=0.7, edgecolor='black', palette=color_pal, zorder=1, alpha=0.7)\n",
    "    \n",
    "    centroids = pcdf.sort_values(by=var).groupby(var).describe()[\"PC1\"][[\"mean\", \"std\"]]\\\n",
    ".join(pcdf.groupby(var).describe()[\"PC2\"][[\"mean\", \"std\"]], lsuffix=\"1\", rsuffix=\"2\")\n",
    "    \n",
    "    centroids[var] = centroids.index.values\n",
    "    sns.scatterplot(data=centroids, x=\"mean1\", y=\"mean2\", hue=var,\\\n",
    "                   legend=False, palette=color_pal, marker=\"D\", s=100, alpha=1, zorder=2,\\\n",
    "                   linewidth=4, edgecolor='k')\n",
    "    \n",
    "    if not plot_title:\n",
    "        pass\n",
    "    else:\n",
    "        plt.title(plot_title+\"\\n\", weight='bold')\n",
    "\n",
    "    plt.xlabel('PC 1 ({} %)'.format(pc1_var))\n",
    "    plt.ylabel('PC 2 ({} %)'.format(pc2_var))\n",
    "\n",
    "    plt.legend(frameon=True, edgecolor='black', facecolor='white')\n",
    "              #bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save:\n",
    "        if out == None:\n",
    "            print(\"Please specifiy an 'out' string as a name\")\n",
    "        else:\n",
    "            plt.savefig(out, dpi=dpi)\n",
    "            print(\"File saved as '{}'\".format(out))\n",
    "    \n",
    "    if loadings:\n",
    "        cdf = pd.DataFrame(pca.components_,columns=trans_df.drop(var, axis=1).columns)\n",
    "        cdct = dict(cdf.loc[0])\n",
    "        comp1 = pd.DataFrame.from_dict(cdct, orient='index', columns=[\"Component1\"])\n",
    "        comp1[\"abs\"] = abs(comp1.Component1)\n",
    "        comp1.sort_values(by=\"abs\", ascending=False, inplace=True)\n",
    "        return comp1.drop(\"abs\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindStatDiff(df, Group1, Group2, alpha=0.05, diff_thresh=1, top_n=20, \n",
    "                 targets=None, save=False, out=None, dpi=200):\n",
    "    \"\"\"\n",
    "    Function to find significant differences between the groups and display the log-ratios\n",
    "    \n",
    "    1) Determines if a sample is normally distributed and then conductes the appropriate pair-wise test.\n",
    "        a) Mann-Whitney U test for non-parametric.\n",
    "        b) Two-Sample T-test for parametric data.\n",
    "    2) Visualizes difference in log-ratios\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: Dataframe containing the transformed data for both groups\n",
    "    group1: name of group1 as it appears in the dataframe\n",
    "    group2: name of group2 as it appears in the dataframe\n",
    "    alpha: cut-off for p-value\n",
    "    top_n: Integer of top differences to display (default: 20)\n",
    "    targets: List of bacteria to focus on\n",
    "    title_label: String for image title\n",
    "    out: String containing the image name\n",
    "    save: Boolean for saving the image\n",
    "    dpi: Integer specifiying the image DPI\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Image display (Top-n differencnes with q-val > alpha and diff > thresh)\n",
    "    Saved image\n",
    "    diff_df: fold-change DF containg only the significantly different Fold-Changes that are higher in Group1\n",
    "    \"\"\"\n",
    "    \n",
    "    # First required function: StatAnalysis\n",
    "    def StatAnalysis(trans_df, group1, group2, alpha=0.05):\n",
    "        \"\"\"Function to find significant differences between the groups\n",
    "\n",
    "        Determines if a sample is normally distributed and then conductes the appropriate pair-wise test.\n",
    "        Mann-Whitney U test for non-parametric.\n",
    "        Two-Sample T-test for parametric data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: Dataframe containing the transformed data for both groups\n",
    "        group1: name of group1 as it appears in the dataframe\n",
    "        group2: name of group2 as it appears in the dataframe\n",
    "        alpha: cut-off for p-value\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dataframe containing the results\n",
    "        \"\"\"\n",
    "        \n",
    "        from scipy import stats\n",
    "        from statsmodels.stats import multitest\n",
    "\n",
    "        df = trans_df.copy()\n",
    "\n",
    "        dct = {} #To store information and turn into DF at the end\n",
    "        #dct[i] = group1_norm, group2_norm, pval\n",
    "\n",
    "        #Test for normality using D'Agostino Omnibus test\n",
    "        for i in tqdm_nb(df.drop(\"Group\", axis=1), desc=\"Conducting statistical analysis testing\"):\n",
    "\n",
    "            try:\n",
    "                if stats.normaltest(df[i][df.Group == group1])[1] > alpha:\n",
    "                    n1 = \"Parametric\"\n",
    "                else:\n",
    "                    n1 = \"Non-parametric\"\n",
    "\n",
    "                if stats.normaltest(df[i][df.Group == group2])[1] > alpha:\n",
    "                    n2 = \"Parametric\"\n",
    "                else:\n",
    "                    n2 = \"Non-parametric\"\n",
    "            except ValueError: # Not enough samples to properly conduct SkewTest\n",
    "                n1,n2 = \"Non-parametric\", \"Non-parametric\"\n",
    "\n",
    "\n",
    "            #Perform correct pair-wise analysis\n",
    "            if n1 == \"Non-parametric\" or n2 == \"Non-parametric\":\n",
    "                try:\n",
    "                    pval = stats.mannwhitneyu(df[i][df.Group == group1].values,\\\n",
    "                                              df[i][df.Group == group2].values)[1]\n",
    "                except ValueError:\n",
    "                    pval = int(1)\n",
    "\n",
    "            else:\n",
    "                pval = stats.ttest_ind(df[i][df.Group == group1].values,\\\n",
    "                                       df[i][df.Group == group2].values)[1]\n",
    "\n",
    "\n",
    "            #Store the data\n",
    "            dct[i] = [n1, n2,\\\n",
    "                      df[i][df.Group == group1].mean(), df[i][df.Group == group2].mean(),pval]\n",
    "\n",
    "        norm1 = \"{}_dist\".format(group1)\n",
    "        norm2 = \"{}_dist\".format(group2)\n",
    "        mean1 = \"{}_mean\".format(group1)\n",
    "        mean2 = \"{}_mean\".format(group2)\n",
    "\n",
    "\n",
    "        #Create the analysis dataframe\n",
    "        adf = pd.DataFrame.from_dict(dct, orient=\"index\",\\\n",
    "                                     columns=[norm1, norm2, mean1, mean2, \"P_value\"])\n",
    "\n",
    "        #Perform FDR correction\n",
    "        adf[\"Q_value\"] = multitest.multipletests(adf.P_value, method=\"fdr_bh\")[1]\n",
    "\n",
    "        print(\"Total bacteria: {}\".format(len(adf)))\n",
    "        print(\"Total samples: {}\\n\".format(len(df)))\n",
    "\n",
    "        print(\"P < 0.05: {}\"\\\n",
    "          .format(len(adf[adf.P_value < 0.05])))\n",
    "\n",
    "        print(\"Q < 0.05: {}\"\\\n",
    "          .format(len(adf[adf.Q_value < 0.05])))\n",
    "\n",
    "        return adf.sort_values(by=\"Q_value\", ascending=True)\n",
    "    \n",
    "    def DiffViz(df, alpha=0.05, top_n=20, targets=None, out=None, save=False, dpi=200, df_out=False):\n",
    "        \"\"\"\n",
    "        Function to calculate and visualize top-n differences between the log-ratio transformed data\n",
    "\n",
    "        Paramters\n",
    "        ---------\n",
    "        df: Dataframe containing the statistical analysis\n",
    "        alpha: q-value cut-off for significance\n",
    "        top_n: Integer of 'top' differences to display (default: 20)\n",
    "        targets: List of bacteria to focus on\n",
    "        title_label: String for image title\n",
    "        out: String containing the image name\n",
    "        save: Boolean for saving the image\n",
    "        dpi: Integer specifiying the image DPI\n",
    "        df_out: Boolean to return the fold-change df\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Image display (Top-n differencnes with q-val > alpha and diff > thresh)\n",
    "        Saved image\n",
    "        fc_df: fold-change DF containg only the significantly different Fold-Changes\n",
    "        \"\"\"\n",
    "\n",
    "        wdf = df.copy() #working df\n",
    "\n",
    "\n",
    "        ######################Create the diffs (log fold-change)#####################################\n",
    "        #1: find the rows containing the means\n",
    "        rm = re.compile(\"_mean\")\n",
    "        mean_list = list(filter(rm.search,list(wdf.columns)))\n",
    "\n",
    "        g1, g2 = mean_list #Store the groups into variables for easy use\n",
    "\n",
    "        #2: Find the differences\n",
    "        wdf[\"Diff\"] = wdf.apply(lambda x:x[g1] - x[g2]\\\n",
    "                                         if x[g1]-x[g2] != 0\\\n",
    "                                         else 0, axis=1)\n",
    "\n",
    "        wdf[\"abs\"] = abs(wdf[\"Diff\"])\n",
    "        wdf.sort_values(by='abs', ascending=False, inplace=True)\n",
    "        wdf.drop(\"abs\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    #     global fc_df # For trouble-shooting\n",
    "        if targets:\n",
    "            fc_df = wdf[wdf.Q_value < 0.05].loc[ctargs].sort_values(by=\"Diff\")\n",
    "            ctargs = []\n",
    "            for t in targets:\n",
    "                if t in wdf[wdf.Q_value < 0.05].index:\n",
    "                    ctargs.append(t)\n",
    "                else:\n",
    "                    print(f\"{t} not significantly different\")\n",
    "        else:\n",
    "            fc_df = wdf[wdf.Q_value < 0.05].head(top_n)\n",
    "\n",
    "        ##################Create the fold-change graph##############################################\n",
    "        mask1 = fc_df.Diff > 0 #Higher in HMP -- Lower in IBD\n",
    "        mask2 = fc_df.Diff < 0 #Lower in HMP -- Higher in IBD\n",
    "\n",
    "        plt.figure(figsize=(22,20))\n",
    "        plt.bar(fc_df.index[mask1], fc_df.Diff[mask1], color='green', edgecolor='black', linewidth=1)\n",
    "        plt.bar(fc_df.index[mask2], fc_df.Diff[mask2], color='red', edgecolor='black', linewidth=1)\n",
    "\n",
    "        #Draw the graph labels\n",
    "        #name1 = g1.split(\"_\")[0]\n",
    "        #name2 = g2.split(\"_\")[0]\n",
    "        plt.title(\"Top-{} log-ratio differences of {} (green) vs {} (red)\".format(top_n, g1, g2),\\\n",
    "                  size=20)\n",
    "        plt.ylabel(\"Log Difference\\n\", size=22)\n",
    "\n",
    "        #Remove the x-ticks\n",
    "        plt.tick_params(axis='x', bottom=False, labelbottom=False)\n",
    "\n",
    "        #Draw the labels and asterisks on the graph\n",
    "        #Pos labels\n",
    "        x = 0.2\n",
    "        for i in fc_df[mask1].index:\n",
    "            s = i.replace(\"_strain\", \"\\nstrain\")\n",
    "            plt.annotate(s, xy=(x,-0.4), rotation=58, ha='right', va='top',\\\n",
    "                        color='k', size=18)\n",
    "\n",
    "            #Add q-value asterisks\n",
    "            q = fc_df.loc[i][\"Q_value\"]\n",
    "            y = fc_df.loc[i][\"Diff\"]\n",
    "\n",
    "            if q < 0.001:\n",
    "                s = \"***\"\n",
    "            elif q < 0.01:\n",
    "                s = \"**\"\n",
    "            else:\n",
    "                s = \"*\"\n",
    "\n",
    "            plt.annotate(s, xy=(x-0.2,y), ha='center', va='bottom', size=18)\n",
    "\n",
    "            x+=1\n",
    "\n",
    "\n",
    "        #Neg labels\n",
    "        for i in fc_df[mask2].index:\n",
    "            s = i.replace(\"_strain\", \"\\nstrain\")\n",
    "            plt.annotate(s, xy=(x-0.5,0.1), rotation=58, ha='left', va='bottom',\\\n",
    "                        color='k', size=18)\n",
    "\n",
    "            #Add q-value asterisks\n",
    "            q = fc_df.loc[i][\"Q_value\"]\n",
    "            y = fc_df.loc[i][\"Diff\"]\n",
    "\n",
    "            if q < 0.001:\n",
    "                s = \"***\"\n",
    "            elif q < 0.01:\n",
    "                s = \"**\"\n",
    "            else:\n",
    "                s = \"*\"\n",
    "\n",
    "            plt.annotate(s, xy=(x-0.2,y-0.25), ha='center', va='top', size=18)\n",
    "\n",
    "            x+=1\n",
    "\n",
    "        #Add note explaining the q-value asterisks\n",
    "        xmin, xmax, ymin, ymax  = plt.axis()\n",
    "\n",
    "        plt.text(x=xmin+2, y=ymin+1, s=\"\"\"* indicates a corrected P-value < 0.05\n",
    "    ** indicates a corrected P-value < 0.01\n",
    "    *** indicates a corrected P-value < 0.001\"\"\", bbox={'facecolor':'wheat'}, size=20)\n",
    "\n",
    "        #Set up the save-option\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save:\n",
    "            if not out:\n",
    "                print(\"Out name required if you wish to save image\")\n",
    "            else:\n",
    "                print(\"Image saved as '{}'\".format(out))\n",
    "                plt.savefig(out, dpi=dpi)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        if df_out:\n",
    "            return wdf[wdf.Diff > 0]\n",
    "    \n",
    "    # Run the first function\n",
    "    stat_df = StatAnalysis(df, Group1, Group2)\n",
    "    \n",
    "    # Run the second function\n",
    "    diff_df = DiffViz(stat_df, alpha, top_n, targets, out, save, dpi, df_out=True)\n",
    "    \n",
    "    # Add in the functions\n",
    "    tfuncs = pd.read_csv(\"DataFiles/AllFoundRoles.csv\", index_col=\"Unnamed: 0\")\n",
    "    diff_roles = diff_df.join(tfuncs)\n",
    "#     diff_roles[\"Roles\"] = diff_roles.MainRole + \"::\" + diff_roles.SubRole\n",
    "#     \n",
    "    return diff_roles.drop([\"P_value\"], axis=1).join(troles[\"Role\"]).fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "troles = pd.read_csv(\"DataFiles/ALL_TIGR_ROLES_DF.csv\", index_col=\"TIGRFAM\")\n",
    "mods = pd.read_csv(\"DataFiles/ModuleClusterDF.csv\", index_col=\"Modules\")\n",
    "clusts = mods.groupby(\"Cluster\").sum().T.join(troles)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DrawPCA(mods, var=\"Cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lmods = mods.copy()\n",
    "Lmods[\"Group\"] = mods.Cluster.apply(lambda x:x if x == \"Left_Cluster\" else \"Others\")\n",
    "\n",
    "Rmods = mods.copy()\n",
    "Rmods[\"Group\"] = mods.Cluster.apply(lambda x:x if x == \"Right_Cluster\" else \"Others\")\n",
    "\n",
    "Bmods = mods.copy()\n",
    "Bmods[\"Group\"] = mods.Cluster.apply(lambda x:x if x == \"Bottom_Cluster\" else \"Others\")\n",
    "\n",
    "Tmods = mods.copy()\n",
    "Tmods[\"Group\"] = mods.Cluster.apply(lambda x:x if x == \"Top_Cluster\" else \"Others\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ldiff = FindStatDiff(Lmods.drop(\"Cluster\", axis=1), \"Left_Cluster\", \"Others\", top_n=50)\n",
    "Tdiff = FindStatDiff(Tmods.drop(\"Cluster\", axis=1), \"Top_Cluster\", \"Others\", top_n=50)\n",
    "Rdiff = FindStatDiff(Rmods.drop(\"Cluster\", axis=1), \"Right_Cluster\", \"Others\", top_n=50)\n",
    "Bdiff = FindStatDiff(Bmods.drop(\"Cluster\", axis=1), \"Bottom_Cluster\", \"Others\", top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================\n",
      "Top 10 Roles in Top cluster (n=8)\n",
      "=================================\n",
      "Unknown: 246\n",
      "DNA metabolism::DNA replication, recombination, and repair: 62\n",
      "Protein fate::Protein and peptide secretion and trafficking: 53\n",
      "Protein synthesis::Ribosomal proteins: synthesis and modification: 49\n",
      "Protein synthesis::tRNA and rRNA base modification: 43\n",
      "Unknown function::General: 34\n",
      "Unknown function::Enzymes of unknown specificity: 31\n",
      "Energy metabolism::Electron transport: 31\n",
      "NA::NA: 28\n",
      "Protein synthesis::tRNA aminoacylation: 27\n",
      "\n",
      "====================================\n",
      "Top 10 Roles in Bottom cluster (n=5)\n",
      "====================================\n",
      "Unknown: 260\n",
      "Unknown function::General: 34\n",
      "Mobile and extrachromosomal element functions::Prophage functions: 31\n",
      "Cellular processes::Sporulation and germination: 30\n",
      "DNA metabolism::DNA replication, recombination, and repair: 28\n",
      "NA::NA: 28\n",
      "Energy metabolism::Amino acids and amines: 27\n",
      "Transport and binding proteins::Cations and iron carrying compounds: 25\n",
      "Biosynthesis of cofactors, prosthetic groups, and carriers::Heme, porphyrin, and cobalamin: 24\n",
      "Cell envelope::Biosynthesis and degradation of surface polysaccharides and lipopolysaccharides: 24\n",
      "\n",
      "====================================\n",
      "Top 10 Roles in Right cluster (n=16)\n",
      "====================================\n",
      "Unknown: 278\n",
      "DNA metabolism::DNA replication, recombination, and repair: 69\n",
      "Protein synthesis::Ribosomal proteins: synthesis and modification: 50\n",
      "Energy metabolism::Electron transport: 45\n",
      "Protein synthesis::tRNA and rRNA base modification: 44\n",
      "Protein fate::Protein and peptide secretion and trafficking: 42\n",
      "Cell envelope::Biosynthesis and degradation of surface polysaccharides and lipopolysaccharides: 38\n",
      "Unknown function::Enzymes of unknown specificity: 37\n",
      "Unknown function::General: 37\n",
      "Transport and binding proteins::Cations and iron carrying compounds: 29\n",
      "\n",
      "===================================\n",
      "Top 10 Roles in Left cluster (n=20)\n",
      "===================================\n",
      "Unknown: 215\n",
      "Cellular processes::Sporulation and germination: 39\n",
      "Unknown function::General: 32\n",
      "DNA metabolism::DNA replication, recombination, and repair: 31\n",
      "Mobile and extrachromosomal element functions::Prophage functions: 29\n",
      "Unknown function::Enzymes of unknown specificity: 23\n",
      "NA::NA: 23\n",
      "Transport and binding proteins::Cations and iron carrying compounds: 20\n",
      "Cellular processes::Chemotaxis and motility: 20\n",
      "Protein synthesis::tRNA and rRNA base modification: 20\n"
     ]
    }
   ],
   "source": [
    "diff_df_list = [(Tdiff, \"Top\", 8), (Bdiff, \"Bottom\", 5), (Rdiff, \"Right\", 16), (Ldiff, \"Left\", 20)]\n",
    "\n",
    "for i in diff_df_list:\n",
    "    title = f\"Top 10 Roles in {i[1]} cluster (n={i[2]})\"\n",
    "    print(\"\\n\" + \"=\" * len(title))\n",
    "    print(title)\n",
    "    print(\"=\" * len(title))\n",
    "    \n",
    "    for t,c in dict(i[0].groupby(\"Role\").count().sort_values(by=\"Others_dist\", \n",
    "                                                            ascending=False).head(10)[\"Others_dist\"]).items():\n",
    "        print(f\"{t}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "376.633px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
